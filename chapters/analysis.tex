\chapter{Data Analysis}
\label{ch:data-analysis}

The initial goal of this master's thesis was concerned with modeling different infection patterns as influenced through \gls{sirna} mediated gene knockdown. By fitting a \gls{glm} model, using the large set of available cellular features, described in section \ref{sec:scf-data}, as predictor variables and the membership to one of two wells as binary response, it was conjectured that it is possible to determine a subset of most influential features. Unfortunately, to present date, this could not be executed in a sensible manner.

In order the make the substantial amount of single cell data, obtained from several large-scale, high throughput \gls{sirna} screens performed by the InfectX consortium, available to an environment for statistical analysis, an R packages was developed, which is described in chapter \ref{ch:singlecellfeatures}. Building on this crucial piece of infrastructure, the current chapter will explore some of the data analysis that was performed, beginning a short introduction into the theory of \glspl{glm} and continuing with preliminary findings that motivate the investigation of desirable normalization schemes. A final outlook on potential improvements will conclude this chapter.

\section{Statistical Models}
Many algorithms for binary classification exist, including decision trees, \glspl{svm}, Bayesian networks, neural networks and \glspl{glm}, some of which have been encountered in previous sections due to their application in infection scoring (see section \ref{sec:infection-scoring}). As neither prediction not classification per se are of main interest, binary logistic regression presents an attractive method due to availability of closed-form coefficient and model statistics. Therefore, modeling of \gls{sirna} effects on cellular features is performed, using the glm function provided by the R stats package \citep{RCoreTeam2015}.

A large number of binary comparisons are possible with the given datasets. In order to focus on attractive wells in the sense that there is reason to assume they might be biologically interesting, the $n \choose 2$ possible combinations of wells (within a single plate, where $n=384$, \tilde 70000 pairs can be formed), are narrowed down using a \gls{pmm} as derived by \cite{Ramo2014}. Of the resulting hit list, several genes are selected and compared to wells containing scrambled \gls{sirna} reagents, which should provide a good choice for usage as control. A possible alternative to scrambled experiments are mock wells but owing to the complete absence of \gls{sirna} molecules, the difference in treatment of cells is only increased and hence it can be argued that they provide biased comparisons.

\subsection{Generalized Linear Models}
Modeling the relationship among variables is one of the most important applications of statistical theory. The study of regression analysis (and the closely related notion of correlation) started to form towards the end of the 19th century with Sir Francis Galton's study of height heredity in humans and his observation of regression towards the mean. Over the next few years, Udny Yule and Karl Pearson cast the developed concepts into precise mathematical formulation, in turn building on work performed by Adrien-Marie Legendre and Carl Friedrich Gauss who developed the method of least squares almost a century earlier \citep{Allen1997}.

A multiple linear regression model can be written in matrix-vector form as
\begin{equation}
  y = X \beta + \epsilon \label{eq:lin-reg}
\end{equation}
where $y \in \R^n$ is the vector of observations on the dependent variable, the design matrix $X \in \R^{n \times p}$ contains data on the independent variables, $\beta \in \R^p$ is the $p$-dimensional parameter vector and the error term $\epsilon \in \R^n$ captures effects not modeled by the regressors.

In order to estimate unknown coefficients $\beta_i$, the ordinary least squares estimator minimizes the residual sum of squares, the squared differences between observed responses and their predictions according to the linear model.
\begin{subequations}
\begin{align}
  \Hbeta &= \argmin_{\beta} \norm{y - X \beta}^2 \\
         &= (X^T X)^{-1} X^T y \label{eq:ols-estimate}
\end{align}
\end{subequations}
Some assumptions are typically associated with linear regression models that yield desirable attributes for the estimates (e.g. statistical tests of significance). None of these restrictions are imposed on the explanatory variables; they can be continuous or discrete and combined as well as transformed arbitrarily. Furthermore, in practice, it is irrelevant whether the covariates are treated as random variables or as deterministic constants. With exception of the field of econometrics it appears that the majority of literature adheres to the latter interpretation and therefore, statements will not explicitly be conditional on covariate values.
\begin{description}
  \item[Linearity.] The relationship between dependent and independent variables is assumed to be linear in coefficients (after suitable transformations of regressors) and individual effects additive. If this cannot be satisfied, a linear model is not suitable.
  \item[Full rank.] For the matrix $X^T X$ to be invertible, it has to have full column rank $p$. Therefore $n \geq p$ and all covariates must be linearly independent.
  \item[Exogeneity.] All independent variables should be known exactly i.e. contain no measurement or observation errors as only the mean squared error of the dependent variable is minimized. Additionally, all important causal factors have to be included in the model. Exogeneity implies $\Erw[\epsilon_i] = 0 \forall i$, as well no correlation between regressors and error terms \citep{Hayashi2000}.
  \item[Spherical errors.] This includes both homoscedasticity or constant error variance, $\Erw[\epsilon_i^2] = \sigma^2 \forall i$, and uncorrelated errors $\Erw[\epsilon_i \epsilon_j] = 0 \forall i \neq j$. These two conditions can be written more compactly as $\Var(\epsilon) = \sigma^2I_{n \times n}$.
  \item[Normality.] For the estimated coefficients to gain some additional desirable characteristics, it can be required that the errors $\epsilon_i$ be jointly normally distributed. Together with the above restrictions on expectation and variance, this yields $\epsilon \mathbin{\sim} \N_n(0, \sigma^2 I_{n \times n})$.
\end{description}

Violations of these assumptions have varying consequences. In situations of perfect collinearity, the ordinary least squares estimator $\Hbeta$ as defined in \eqref{eq:ols-estimate} does not exist. Recovering such a situation is possible by using a generalized matrix inverse (for example the Moore--Penrose pseudoinverse) or by dropping the corresponding variables. High correlation inflates coefficient variances, which may be countered by employing a regularization scheme like ridge regression. Failure to fulfill exogeneity, for example by omitting relevant explanatory variables, leads to regressors that are correlated with the error term, which in turn yields invalid (biased and inconsistent) \gls{ols} estimates. The method of instrumental variables can help to produce an unbiased estimator nonetheless.

The assumption of spherical errors ensures that the least squares estimator is the best linear unbiased estimator in the sense that it has minimal variance among all linear unbiased estimators. Heteroscedasticity and autocorrelation do not yield biased coefficient estimates but can introduce bias in \gls{ols} estimates of variance, causing inaccurate standard errors. Such a situation calls for generalized least squares estimation, for example weighted least squares when the data is not homoscedastic ($\sigma^2$ is vector-valued but off-diagonal elements of $\Var(\epsilon)$ are zero), or feasible generalized least squares which can be applied in case of heteroscedasticity and\slash or correlation between errors. Whenever the condition of spherical errors does not hold, \gls{ols} is inefficient and generalized least squares estimators have smaller variance.

Finally, normality provides the framework necessary for applying common hypothesis testing, yielding t-statistics, p-values and confidence intervals for coefficients, as well as an F-statistic for the model as a whole. Furthermore, under normality assumptions, the \gls{ols} estimator and \gls{mle} coincide. Quantile regression and other forms of robust regression, such as M estimators may restore validity of inference when errors do not follow a normal distribution.

Modeling data to a dichotomous response variable with \gls{ols} methodology, while readily possible, often is a bad choice due to violations of several of the above assumptions. First of all, normal distributions are continous with support $x \in \R$ and thusly a categorical variable cannot be normally distributed. Homoscedasticity does not hold either, as can easily be seen in a geometric argument: any line with nonzero slope, fitted to a set of $Y$ with $Y_i \in \{0,1\}$, will produce residuals that vary linearly. Finally and perhaps most importantly, a linear model is unsuitable due to missing constraints on the range of fitted values. While this by itself could be dealt with, the concept of additivity within this context raises questions of its own when fitted values are thought of as probabilities. Linear behavior throughout the range of 0 (impossible) to 1 (certain) makes little sense for most practical applications, as typically, some flattening is expected when approaching either end of the spectrum.

In view of the above considerations it becomes clear that some sort of extension to ordinary linear regression is needed in order to deal with binary response variables. A theory, unifying several previously separately treated statistical models, including linear regression, \gls{anova}, logistic regression, Poisson regression and multinomial response, as \acrfullpl{glm}, was developed by John Nelder and Robert Wedderburn in the early 1970's \citep{Nelder1972}. Much of this section is based on that work.

In order to accommodate the newly added extensions, the classical linear model is rewritten as

\begin{equation}
  \Erw[Y] = \mu \text{ where } g(\mu) = \eta \text{ and } \eta = X \beta,
\end{equation}

yielding a three-part specification, consisting of

\begin{enumerate}[label=(\alph*)]
  \item the \textit{random component}; $Y$ is distributed according to a member of the exponential family,\footnote{Many of the predominately used distributions belong to the exponential family, including but not restricted to Bernoulli, binomial, Poisson, exponential and normal distributions. Common to all members, the probability density function can be written as
  \begin{equation}
    f(x;\theta) = h(x) e^{\theta^\intercal T(x)-A(\theta)}
  \end{equation}
  where $\theta$ is the vector of parameters, $T(x)$ the vector of sufficient statistics, $A(\theta)$ the cumulant generating function and $h(x)$ the base measure. In case of the Bernoulli distribution
  \begin{equation}
    f(k;\pi) = \pi^k (1-\pi)^{1-k}\label{eq:bern-pmf}
  \end{equation}
  this gives $h(k) = 1$ $T(k) = k$, $\theta = \log\frac{\pi}{1-\pi}$ and $A(\theta) = \log(1+e^\theta)$. Restricting \glspl{glm} to exponential family distributions makes it possible to stay within the framework of maximum likelihood parameter estimations, as given this restriction, \gls{mle} yields the best parameter estimator with respect to minimal variance.}
  \item the \textit{systematic component}; a linear predictor is given by $\eta = X\beta$,
  \item and the \textit{link} between random and systematic components, expressed as the link function $g(\cdot)$, such that $\eta = g(\mu)$; in case of normally distributed Y, $\mu = \eta$ (identity link).
\end{enumerate}

\renewcommand{\arraystretch}{2}
\setlength{\tabcolsep}{0.2em}
\begin{table}
  \centering
  \caption[Link functions of common univariate distributions of the exponential family.]{Common univariate distributions of the exponential family alongside mean and canonical link functions.}
  \label{tab:glm-links}
  \footnotesize
  \begin{tabular}{L{0.18\linewidth}L{0.18\linewidth}L{0.18\linewidth}L{0.18\linewidth}L{0.18\linewidth}}
    Distribution &
      Support &
      Link name &
      Link function &
      Mean function \\
    \hline 
    Normal &
      $(-\infty, +\infty)$ &
      identity &
      $\eta = \mu$ &
      $\mu = X\beta$\\
    Poisson &
      $\{0, 1, 2, \dotsc\}$ &
      log &
      $\eta = \log(\mu)$ &
      $\mu = e^{X\beta}$\\
    Binomial &
      $\{0, 1, \dotsc, N\}$ &
      logit &
      $\eta = \log\left(\frac{\mu}{1-\mu}\right)$ &
      $\mu = \frac{e^{X\beta}}{1+e^{X\beta}}$\\
    Gamma &
      $(0, +\infty)$ &
      reciprocal &
      $\eta = -\mu^{-1}$ &
      $\mu = -(X\beta)^{-1}$\\
    Inverse Gaussian &
      $(0, +\infty)$ &
      inverse squared &
      $\eta = -\mu^{-2}$ &
      $\mu = (-2X\beta)^{-1/2}$\\
  \end{tabular}
\end{table}

Mean and canonical link functions for several common univariate distributions belonging to the exponential family are shown in table \ref{tab:glm-links}. Binary response can be interpreted as 
\begin{equation}
  Y_i \mathbin{\sim} \Bern(\pi_i)
\end{equation}
with
\begin{equation}
  \pi_i = \P(Y_i=1) = 1 - \P(Y_i=0).
\end{equation}
Therefore, $\pi_i$ can be thought of as the probability of one outcome (i.e. success) and its complement ($1-\pi_i$) corresponds to the probability of the other outcome (failure). Each experimental unit that yields one outcome is associated with a vector of independent variables $x_i = (x_{i,1}, x_{i,2}, \dotsc, x_{i,p})$ and the goal of regression in this context becomes modeling the relationship between response probability $\pi_i$ and explanatory variables $x_{i}$. A suitable link function for Bernoulli distributed response, as discussed above, is required to map the unit interval onto the whole real line and preferably be shaped sigmoidally in order accurately describe increasingly likely as well as increasingly unlikely events. Three possibilities are often considered within this context,

\begin{enumerate}[label=(\alph*)]
  \item the \textit{logit} or logistic function
  \begin{equation}
    g(\pi) = \log\left(\frac{\pi}{1-\pi}\right),
  \end{equation}
  \item the \textit{probit} or inverse normal function
  \begin{equation}
    g(\pi) = \Phi^{-1}(\pi),
  \end{equation}
  with $\Phi(\cdot)$ denoting the cumulative distribution function of the normal distribution,
  \item and the \textit{complementary log-log} function
  \begin{equation}
    g(\pi) = \log\left(-\log(1-\pi)\right).
  \end{equation}
\end{enumerate}

All three functions are continuous and increasing on $(0,1)$ and the first two are symmetric in the sense that $g(\pi) = -g(1-\pi)$, while the third is not. Using a logit link function, the probability of a positive response can therefore be written as
\begin{equation}
  \pi_i = \frac{e^{x_i^\intercal \beta}}{1+e^{x_i^\intercal \beta}}
\end{equation}
and the model is, slightly rearranged, stated as
\begin{equation}
  \log\left(\frac{\pi_i}{1-\pi_i}\right) = x_i^\intercal \beta = \sum_{j=1}^p x_{i,j}\beta_j.\label{eq:log-model}
\end{equation}

where $x_i$ is shorthand for the $i$-th row $(x_{i,1}, x_{i,2}, \dotsc, x_{i,p})$ of the design matrix $X$. Consequently, of a unit change in a covariate $x_{i,j}$ will increase the corresponding probability log-odds by a multiplicative factor $\exp(\beta_j)$, as can easily be seen by exponentiating expression \ref{eq:log-model}.

The likelihood of a set of parameter values $\pi$, given the data $y$, is equal to the probability of the data, given the parameters. Hence, in a logistic regression model (see expression \ref{eq:bern-pmf} for the probability mass function of a Bernoulli distribution), we have

\begin{subequations}
\begin{align}
  L(\beta; y) = \P(y \given \beta) &= \prod_{i=1}^n \pi_i^{y_i} (1-\pi_i)^{1-y_i} \label{eq:lik-any} \\
  &= \prod_{i=1}^n \frac{e^{x_i^\intercal \beta y_i}}{1+e^{x_i^\intercal \beta}} \label{eq:lik-logit}
\end{align}
\end{subequations}

and expressed, for reasons of convenience, as log-likelihood

\begin{subequations}
\begin{align}
  l(\beta; y) &= \sum_{i=1}^n y_i \log(\pi_i) + (1-y_i) \log(1-\pi_i) \label{eq:loglik-any} \\
  &= \sum_{i=1}^n x_i^\intercal \beta y_i -  \log\left(1+e^{x_i^\intercal \beta}\right). \label{eq:loglik-logit}
\end{align}
\end{subequations}

Both expressions \ref{eq:lik-logit} and \ref{eq:loglik-logit} are derived using the logit link function but any other of the proposed links can be used to substitute $\pi_i$ in equations \ref{eq:lik-any} and \ref{eq:loglik-any}. In order to find the maximum likelihood estimates
\begin{equation}
\Hbeta = \argmax_{\beta} l(\beta; y), 
\end{equation}

the first derivative of $l(\beta; y)$ with respect to $\beta_j$ in required:

\begin{subequations}\label{eq:firstder}
\begin{align}
  \frac{\partial l(\beta)}{\partial \beta_j} &= \sum_{i=1}^n \frac{y_i - \pi_i}{\pi_i (1-\pi_i)} \frac{d \pi_i}{d \eta_i} \frac{\partial \eta_i}{\partial \beta_j} \label{eq:firstder-any} \\
  &= \sum_{i=1}^n \left(y_i - \frac{e^{x_i^\intercal \beta}}{1+e^{x_i^\intercal \beta}}\right) x_{i,j} . \label{eq:firstder-logit}
\end{align}
\end{subequations}

Again, \ref{eq:firstder-logit} is obtained from \ref{eq:firstder-any} by using a logit link, and consequently substituting 

\begin{equation*}
  \frac{d \pi_i}{d \eta_i} = \frac{-e^{-\eta_i}}{(1+e^{-\eta_i})} \text{,\ \ } \eta_i = x_i^\intercal \beta \text{\ \ and\ \ } \frac{\partial \eta_i}{\partial \beta_j} = x_{i,j}.
\end{equation*}

The log-likelihood maximum is found by setting the first derivatives in \ref{eq:firstder} to zero and solving for $\beta$. No closed form solutions to the resulting equations exist and therefore numerical algorithms such as Newton-Raphson are usually used, which under the given circumstances can be formulated as an \gls{irls} regression problem. Newton's method attempts to find the root $x_r$ of a differentiable function $f(x)$ by starting with an initial guess $x_0$ and iterating

\begin{equation}
  x_{n+1} = x_n - \frac{f(x_n)}{f^\prime(x_n)}.
\end{equation}

When a good initial guess is made and some restrictions on $f(x)$ apply, the rate of convergence is quadratic\footnote{Given a function$f(x)$, that is three-times differentiable in an interval $I^\ast= [a,b]$ such that $a < x_r < b$ and $f^\prime(x_r) \neq 0$, there exists an interval $I = [x_r-\delta, x_r+\delta]$, with $\delta > 0$ for which every $x_0 \in I$ converges quadratically towards $x_r$ \citep{Schwarz2006}.} and even if certain conditions do not hold, or the starting point is not suitably chosen, the algorithm may still converge towards the root. Returning to maximum likelihood notation of expression \ref{eq:firstder}, a Newton-Raphson iteration can be written as

\begin{equation}
  \beta^{(n+1)} = \beta^{(n)} - \frac{l^\prime(\beta^{(n)})}{l^{\prime\prime}(\beta^{(n)})}.\label{eq:nr-loglik}
\end{equation}

The second derivative of the log likelihood with respect to $\beta$, as needed in \ref{eq:nr-loglik} is

\begin{equation}
  \frac{\partial^2 l(\beta)}{\partial \beta_j\partial \beta_k} = -\sum_{i=1}^n \frac{1}{\pi_i (1-\pi_i)} \left(\frac{d \pi_i}{d \eta_i}\right)^2 \frac{\partial \eta_i}{\partial \beta_j} \frac{\partial \eta_i}{\partial \beta_k},
\end{equation}

which, in matrix form, can be written as

\begin{equation}
   \frac{\partial^2 l(\beta)}{\partial \beta\partial \beta^\intercal} = -X^\intercal W X,
\end{equation}

with the $n \times n$ diagonal matrix $W$

\begin{equation*}
  W = \diag\left\{\frac{1}{\pi_i (1-\pi_i)} \left(\frac{d \pi_i}{d \eta_i}\right)^2\right\} \text{,\ \ as\ \ } \frac{\partial \eta_i}{\partial \beta_j} = x_{i,j} \text{\ \ and \ \ } \frac{\partial \eta_i}{\partial \beta_k} = x_{i,k}.
\end{equation*}

In the case of logistic regression, $W$ simplifies to $w = \diag\{\pi_i (1-\pi_i)\}$ and $l^\prime(\beta)$ can be written in matrix form as $l^\prime(\beta) = X^\intercal (y - \pi)$.

Using these results, expression \ref{eq:nr-loglik} is rewritten as

\begin{equation}
  \beta^{(n+1)} = \beta^{(n)} + \frac{X^\intercal (y - \pi)}{X^\intercal W X},
\end{equation}

which is iterated until the updates become smaller than a threshold and convergence is said to have been reached. In order to reformulate Newton-Raphson as an \gls{irls} problem, $\beta^{(n)}$ is substituted by

\begin{equation*}
  \beta^{(n)} = \frac{X^\intercal W X}{X^\intercal W X} \beta^{(n)},
\end{equation*}

yielding

\begin{subequations}
\begin{align}
  \beta^{(n+1)} &= \left(X^\intercal W X\right)^{-1} X^\intercal W \left(X \beta^{(n)} + W^{-1} (y - \pi)\right) \\
  &= \left(X^\intercal W X\right)^{-1} X^\intercal W z \label{eq:irls}
\end{align}
\end{subequations}

where expression \ref{eq:irls} corresponds to a weighted least squares problem with response

\begin{equation*}
  z = X \beta^{(n)} + W^{-1} (y - \pi).
\end{equation*}

Therefore each iteration of equation \ref{eq:irls} solves weighted regression on a transformed version of the response, called the adjusted dependent variable. At each step, the current estimate of $\beta$, $\beta^{(n)}$, is used to compute new weights $W$ and consequently a new value for the adjusted variable $z$, which in turn is used for computing the minimizer for

\begin{equation}
  \beta^{(n+1)} = \argmin_{\beta} \left(z-X\beta^{(n)}\right)^{-1} W \left(z-X\beta^{(n)}\right),
\end{equation}

providing new values $\beta^{(n+1)}$.

%\subsection{Regularization}
%Two regularization schemes, ridge regression and lasso regression, will be introduced very briefly due to their availability in glmnet.
% esl p61

\subsection{Parallel Mixed Model}
As a way of exploiting multi-tier replication, typically involved in \gls{sirna} screening, starting at the level of individual sequences, different sequences targeting the same gene and as is the case for InfectX, different treatments in the form of varying pathogens, \citeauthor{Ramo2014} developed a \acrfull{pmm}, capable of gaining statistical power from structural parallelism. All following remarks are adapted from the referenced publication and the topic is only outlined briefly for completeness sake. The interested reader is directed to \cite{Ramo2014} for more information. The model as proposed is written as

\begin{equation}
  y_{pgs} = \mu_p + a_g + b_{pg} + \epsilon_{pgs},
\end{equation}

where $y_{pgs}$ represents the per-well phenotype (e.g. infection score), which is described as the sum of a fixed effect $\mu_p$ for pathogen $p$, as well as random effects $a_g$ for gene $g$, $b_{pg}$, a correction for gene $g$ within pathogen $p$, and an error term $e_{pgs}$. The overall effect of gene knockdown $g$ under pathogen treatment $p$ therefore is 

\begin{equation}
  c_{pg} = a_g + b_{pg},
\end{equation}

and a positive estimated effect corresponds to enhanced infection levels, while a negative $c_{pg}$ value indicates reduced infectivity. Random effects are distributed as $a_g \mathbin{\sim} \N(0, \sigma_a^2)$, $b_{pg} \mathbin{\sim} \N(0, \sigma_b^2)$ and $\epsilon_{pgs} \mathbin{\sim} \N(0, \sigma_\epsilon^2)$, while estimation is carried out by the CRAN package lme4, using restricted maximum likelihood. Hits are selected according to an estimated local \gls{fdr} which assumes that a mixture of two distributions corresponding to genes that are no hits ($f_0$) and genes that actually are hits ($f_1$), generates the overall distribution. Furthermore, it is surmised that $f_0 \mathbin{\sim} \N(0, \sigma_a^2+\sigma_b^2)$ and $f_0 \mathbin{\sim} \N(\theta, \sigma_a^2+\sigma_b^2)$, i.e. the two distributions are identical except for a shift in mean by $\theta$. The overall distribution can be expressed as $f(c_{pg}) = \pi_0 f_0(c_{pg}) + (1-\pi_0) f_1(c_{pg})$, where $\pi_0$ is the proportion of true hits. Finally, the \gls{fdr} is defined as

\input{R/pmm}

\begin{equation}
  \fdr(c_{pg}) = q_{pg} = \frac{\pi_0 f_0(c_{pg})}{f(c_{pg})}
\end{equation}

and represents the probability that the effect for a given gene and pathogen is a false discovery. Figure \ref{fig:pmm-heatmap} displays a visualization obtained by applying the PMM package (available on Bioconductor) to kinome-wide InfectX screens. Color-coding corresponds to estimated $c_{pg}$ effects and columns are sorted according to descending mean values. Only genes are included where the estimated \gls{fdr} is below 0.4 for at least one pathogen (indicated by black borders), suggesting that up to 40\% of individual hits may be superfluous. Centered white dots indicate the maximum and minimum $c_{pg}$ value for each pathogen. For each gene, a sharedness score is displayed as well. This quantity is defined as

\begin{equation}
  s_g = \frac{1}{2} \left(\left(1-\frac{1}{P}\sum_{p=1}^P q_{pg}\right) + \frac{\sum_{p=1}^P \mathds{1}_{q_{pg} < 1}}{P}\right)
\end{equation}

and quantifies how common a hit gene is among pathogens by describing both the extent of downward shift from 1 of the mean $q_{pg}$ value (over all $P=8$ pathogens), as well as the fraction of pathogens that contribute $q_{pg} < 1$ instances.

\section{Preliminary Findings}
Due to the high degree of redundancy in feature extraction during image analysis, a significant amount of correlation among features can be expected. Measurements of objects describing similar image segments, as well as features that build on related concepts, such as mean, median or integrated intensities, will obviously yield similar values and cause a dependence structure that has to be dealt with in statistical analysis. Figure \ref{fig:analysis-correlation} visualizes the issue by showing a heatmap representation of a correlation matrix, obtained from all available \textit{AreaShape}, \textit{Intensity} and \textit{Texture} features for a \textit{Brucella} plate using Dharmacon \gls{sirna} (J110-2D) and a randomly sampled subpopulation of cells (10\%).

The three feature groups can easily be spotted as diagonal blocks with high within group and lower between group correlation, and the situation is worst for intensity features due to the extensive set of measurements quantifying similar properties (see figure \ref{fig:intensity-features}). The texture segment is subdivided into four groups, corresponding from bottom left to top right to \textit{Cells}, \textit{Nuclei}, \textit{PeriNuclei} and \textit{VoronoiCells}. Absent off-diagonal correlation between nuclear and perinuclear regions is due to mutual exclusivity, whereas the off-diagonal correlation of cell and Voronoi cell features is due to significant overlap.

\input{R/correlation-heatmap}

In order to deal with this data issue, features are transformed to the coordinate system of \glspl{pc} prior to \gls{glm} model fitting. When employing \gls{pca}, typically the first 10\% (or 30--50) of \glspl{pc} capture around 90\% of the overall variance in the data, corroborating the above claim of significant correlation among feature vectors. A further problem that is present in many datasets is that pairs of wells can be perfectly separated. This causes problems in maximum likelihood estimation, as affected coefficients are allowed to grow arbitrarily large, but is not unexpected given the large design space. \Gls{pca} provides a tool for addressing the issue by encouraging only inclusion of a subset of \glspl{pc} and thus reducing dimensionality.

Several \gls{glm} model fits, based on principal component regression are summarized in table \ref{tab:glm-1}. For the same plate is used in figure \ref{fig:analysis-correlation} (J110-2D, \textit{Brucella}, Dharmacon unpooled, replicate 1), well pairs corresponding to the genes identified by \gls{pmm} as down-hits, MTOR (H6) and PIK3R3 (K8), as well as up-hits, RIPK4 (G17) and TGFBR1 (M4), are formed with all available scrambled wells of the given plate. In order to establish a baseline of sorts, scrambled wells are also paired with each other.

For describing how well the discrepancy between the two input wells is captured, some model characteristics, alongside scores for predictions, are reported. The AIC is a goodness of fit estimate, constituting of the maximized log-likelihood $l$, as well as a penalty term for model size $k$, and is defined as

\begin{equation}
  \aic = 2k - 2l(\Hpi; y).
\end{equation}

Deviance values require caution when interpreted as a goodness-of-fit criterion, especially when used as an absolute measure, rather than being employed in a comparative capacity for nested models (analysis of deviance). The glm function of the R stats package reports both null deviance, defined as

\begin{equation}
  D^{(0)}(y; \pi^{(0)}) = 2l(\widetilde{\pi}; y) - 2l(\pi^{(0)}; y),
\end{equation}

and residual deviance as

\begin{equation}
  D(y; \Hpi) = 2l(\widetilde{\pi}; y) - 2l(\Hpi; y).
\end{equation}

The saturated model $\widetilde{\pi}$ contains as many parameters as there are data observations ($n$) and consequently represents the maximally possible likelihood. The null model $\pi^{(0)}$ contains only an intercept term (i.e. $y = constant$), while the proposed model $\Hpi$ attempts to explain the data using $p+1$ parameters (one for each covariate and an intercept). Finally, the values reported in table \ref{tab:glm-1} as $\Delta\textsubscript{deviance}$ are obtained as

\begin{equation}
  \Delta\textsubscript{deviance} = D^{(0)}(y; \pi^{(0)}) - D(y; \Hpi),
\end{equation}

therefore describing the difference between the quality of fit of the null model to that of the estimated model. Similarly, for the degrees of freedom, 

\begin{equation}
  \Delta\textsubscript{df} = \text{df}_\text{null} - \text{df}_\text{res} = n-1 -(n-(p+1)) = p.
\end{equation}

Under certain assumptions,\footnote{Deviance is only distributed as $\chi^2$ in the limit where for each $i \in \{1, 2, \dotsc, n\}$, the number of identical covariate rows $x_i$ grows to infinity. For continuous regressors, this is typically not the case, as the number of unique $x_i$ will often be very close to $n$. In case of binomially distributed response, the possibility of over-dispersion causes additional issues, which are not further described, as this does not apply to the current situation. Nevertheless, \citeauthor{Nelder1972} state that ``[t]he $\chi^2$ approximation is usually quite accurate for differences of deviances even though it is inaccurate for the deviances themselves.''} $\Delta\textsubscript{deviance} \mathbin{\sim} \chi^2_p$, and therefore a p-value for the significance of the model fit can be calculated. This is not reproduced, as all fits provide highly significant evidence against the null hypothesis, which assumes the fitted model to be no better than the null model.

\renewcommand{\arraystretch}{1.5}
\setlength{\tabcolsep}{0.25em}
\begin{table}
  \centering
  \caption[\Gls{glm} model summaries based on principal components for several well pairings.]{Summaries of several \gls{glm} models obtained by pairing wells corresponding to the genes MTOR (H6), PIK3R3 (K8), RIPK4 (G17) and TGFBR1 (M4) with all available scrambled wells on the same plate (J110-2D). Comparisons among scrambled wells serve as baseline (the scrambled row corresponds to well G1). Model fit is summarized by \gls{aic}, the difference in deviance and degrees of freedom (both between null and fitted models), as well as prediction scores. The following models suffer from separated data: MTOR (A24, G1, J2), PIK3R3 (E24, G1, G23, H2, L23), RIPK4 (E2, G23, H2, J2, L1) and Scrambled (A24, G23, H24, J24, L23).}
  \label{tab:glm-1}
  \footnotesize
  \vspace{5px}
  \input{R/glm1}
\end{table}

Moving along to prediction scores, table \ref{tab:glm-1} shows both accuracy and \gls{mcc} values obtained by separating 20\% of data for each group from training data and evaluating predictions using test data. Accuracy is defined as

\begin{equation}
  \text{Acc} = \frac{n_{tp} + n_{tn}}{n_p+n_n}
\end{equation}

where $n_{tp}$ represents the number of true positives, $n_{tn}$ the count of true negatives and $p$, $n$ the number of positive and negative instances, respectively. The \acrshort{mcc} \citep{Matthews1975} can be evaluated as

\begin{equation}
  \text{Mcc} = \frac{n_{tp} n_{tn} - n_{fp} n_{fn}}{\sqrt{(n_{tp} + n_{fp})(n_{tp} + n_{fn})(n_{tn} + n_{fp})(n_{tn} + n_{fn})}}
\end{equation}

and $n_{fp}$, $n_{fn}$ correspond to false positives and false negatives. Values for \gls{mcc} range from $-1$ (total disagreement) to $1$ (perfect prediction), and the midpoint $0$ indicates random prediction.

Given these model characteristics, two patterns emerge: (1) the ability to distinguish two wells is dependent on well distance within the plate and (2) the differences among scrambled wells, in terms of computed quality of fit estimates, are comparable to those that are observed when modeling the discrepancy between hit genes and control wells. To make the first claim, well locations are required: MTOR (H6), PIK3R3 (K8), RIPK4 (G17), TGFBR1 (M4) and Scrambled (A2, A24, E2, E24, G1, G23, H2, H24, J2, J24, L1 and L23). For both MTOR and TGFBR1, which represent early-row, down-hit genes, an alternating sequence is clearly discernible and is characterized by lower AIC, larger $\Delta\textsubscript{deviance}$ and better predictive power for wells that are closer together, while the opposite holds for comparisons among scrambled wells. The effect is less distinct for PIK3R3 and RIPK4 which both are located more towards the plate center and are identified as up-hits. For RIPK4 the alternations are still noticeable albeit, as in scrambled, polarity is reversed. This is indicative of some technical artifacts contained in the data, that dominate biological features of interest. Furthermore, the excellent predictions that can be made based on membership to either one of a scrambled well pair is disconcerting, as biologically they should be equivalent. Again, technical effects seemingly dominate.

In 18 of the 59 models displayed in table \ref{tab:glm-1}, a warning regarding perfectly separated data is issued by the glm routine (in the example of MTOR, wells A24, G1 and J2). For this particular case, the matter is not further investigated, as it does not have obviously relevant consequences.\footnote{For other inquiries, not reported here, perfect separation was addressed by reducing the number of \glspl{pc} or using regularized procedures. Furthermore the issue was studied by solving associated linear programming problems in order to explicitly find the separating hyperplane and thus determine whether this is actually part of the data or caused by numerical shortcomings of the glm implementation. In an example by \citeauthor{Gelman2015} it is shown that a response such as \mintinline{text}{y <- rep (c(1,0),c(10,5))} in a model containing only an intercept, will trigger the separation warning dependent on starting values (i.e. using \mintinline{text}{start=2.6}, glm runs fine, whereas \mintinline{text}{start=2.7} issues the warning). A further effect known to cause problems under certain circumstances, especially for convergence, is the Hauck-Donner phenomenon. As it turns out, separating hyperplanes can reliably be determined, most probably owing in part to the high dimensional setting (with respect to predictor variables).} Affected data points appear in line with well pairs where no complete data separation is possible and the above arguments still hold if possibly questionable data is excluded (albeit patterns are less clearly distinguishable).

Apart from the results shown, many similar investigations were performed, using other datasets and\slash or slightly different methods. Further \textit{Brucella} plates were considered, pooled \gls{sirna} experiments, libraries from Ambion and Qiagen, as were several \textit{Salmonella} plates and for some inquiries, cell population was limited to infected only. Method-wise, ridge and elastic net penalized regression (glmnet), other glm implementations, such as glm2 (\cite{Marschner2014}; uses a more robust fitting procedure), brglm (\cite{Kosmidis2013}; deals with data separation by penalized maximum likelihood) and bayesglm (\cite{Gelman2015}; regularizes coefficients though a weakly informative prior distribution), as well as step-wise model building (using the step function of the R stats package) was explored.

% bootstrapped stuff

All analysis performed clearly indicates that for the intended type of modeling, an effective normalization scheme has to be developed that is able to capture technical effects (and perhaps even spurious biological artifacts), without destroying phenotypic information coming from gene knockdown and pathogen infection. Attempts of achieving this are outlined in the following section.

\section{Data Normalization}
\label{sec:data-normalization}
The large amount of technical variation, coupled with treating biological systems, which in turn are associated with their own inherent noise, make the analysis of \gls{hts} data a challenging endeavor, requiring thorough normalization methods. The poor reproducibility that often afflicts \gls{sirna} experiments may in part be addressed and even resolved with the development of effective corrections that do not negatively affect phenotypes of interest. The following sections outline two types of normalization approaches, plate and well level corrections via Z-scoring\slash B-scoring and using residuals of a \gls{mars} model at single cell level, as well as the application of such procedures to InfectX datasets.

\subsection{Plate and Well Level Normalization}
In order to correct \gls{sirna} data for experimental artifacts at plate and well levels, two schemes have become standard practice, Z-scoring and B-scoring \citep{Malo2006}. The former is widely known (outside the field of \gls{sirna} analysis) and is defined as

\begin{equation}
  z_{ij} = \frac{x_{ij} - \mu_j}{\sigma_j}, \; i=1, \dotsc, n \: \text{and} \: j=1, \dotsc, m
\end{equation}

where $n$ is the number of cells, $m$ the number of features and $x_{.j}$ the data vector of feature j to be normalized, while $\mu_j$ represents the sample mean and $\sigma_j$ the sample standard deviation of feature j, as

\begin{equation}
  \mu_j = \frac{1}{n} \sum_{i=1}^n x_{ij} \; \text{and} \; \sigma_j = \sqrt{\frac{1}{n-1} \sum_{i=1}^n (x_{ij}-\mu_j)^2}.
\end{equation}

Applying Z-scoring therefore both centers data around zero and scales dispersion to unit variance. B-scoring is more domain specific and deals with row and column effects that have been discussed previously (e.g. pipetting issues, leading to horizontal patterns or temporal effects such as decay of actin stain intensity, resulting in a vertically oriented gradient, when imaging is performed column-wise). B-scoring can be expressed as

\begin{equation}
  b_{rcp} = \frac{\epsilon_{rcp}}{\mad(r)} = \frac{x_{rcp} - (\Hmu_p + \widehat{\alpha}_{rp} + \widehat{\beta}_{cp})}{\mad(r)}, \; r=1, \dotsc, N \: \text{and} \: c=1, \dotsc, M,
\end{equation}

where $N$ is the number of plate rows and $M$ the number of plate columns (in the present setup 16 and 24 respectively). Estimates for row and column effects, $\widehat{\alpha}_{rp}$ and $\widehat{\beta}_{cp}$, are obtained by fitting a two-way median polish algorithm. Together with an estimate for plate average $\Hmu_p$, these three parameters are used to determine a residual $\epsilon_{rcp}$, which divided by the \gls{mad} over the whole plate, yields the B-scored value $b_{rcp}$. Median polishing proceeds by augmenting the plate layout with an additional column and row as

\begin{equation*}
  \begin{array}{cccc|c}
    \epsilon_{1,1,p} & \epsilon_{1,2,p} & \cdots & \epsilon_{1,M,p} & \alpha_{1,p} \\
    \epsilon_{2,1,p} & \epsilon_{2,2,p} & \cdots & \epsilon_{2,M,p} & \alpha_{2,p} \\
    \vdots  & \vdots  & \ddots & \vdots & \vdots \\
    \epsilon_{N,1,p} & \epsilon_{N,2,p} & \cdots & \epsilon_{N,M,p} & \alpha_{N,p} \\
    \hline
    \beta_{1,p} & \beta_{2,p} & \cdots & \beta_{M,p} & \mu_{p} \\
  \end{array}
\end{equation*}

and initializing the values as $\epsilon_{rcp} = x_{rcp}$ and $\alpha_{rp} = \beta_{cp} = \mu_p = 0$. A row sweep consists of iterating all rows, calculating the median of $(\epsilon_{i,1,p}, \epsilon_{i,2,p}, \dotsc, \epsilon_{i,M,p})$ for each row $i$, subtracting the resulting value from $(\epsilon_{i,1,p}, \epsilon_{i,2,p}, \dotsc, \epsilon_{i,M,p})$ and adding it to $\alpha_{i,p}$. The same procedure is also applied to the column effect row where the median of $(\beta_{1,p}, \beta_{2,p}, \dotsc, \beta_{M,p}$ is subtracted from $(\beta_{1,p}, \beta_{2,p}, \dotsc, \beta_{M,p}$ and added to $\mu_p$). A column sweep is carried out analogously and the two procedures are alternated until all rows and columns of residuals have median zero, as do the vectors of row and column effects (or fall below a threshold close to zero). Usually only a couple of sweeps in each direction (\tilde 2) are needed. The results are non-unique as they depend on whether row or column sweeps are put first. Furthermore, using means instead of medians yields a least squares decomposition as in two-way \gls{anova} without iteration, which is less robust towards outliers \citep{Brown2004,Venables2002}. The \gls{mad} is defined as

\begin{equation}
  \mad(x) = \median\left(\abs{x_k-\median(x)}\right).
\end{equation}

and therefore provides an estimation of data spread which is more robust towards outliers as other measures of dispersion, such as standard deviation.

While B-scoring has proven to be a capable normalization tool for data coming from a plate reader or phenotypic data like infection scores as generated from InfectX screens, where there is a single value per well to be adjusted for experimental artifacts, the situation for single cell feature data is much more complex. Ideally, Z-scoring and B-scoring are able to correct for issues at plate and well level, but data hierarchy goes beyond that for InfectX datasets. For example, data can be split into images which are captured individually, possibly with different imaging parameters and therefore may require differing treatment.

\subsection{Multivariate Adaptive Regression Splines}
At cellular level of granularity, an abundance of additional sources of noise may directly be addressed. This includes technical issues, as well as biological variability. Examples for the former are location of cell within the well which might be relevant due some degree of curvature of the well bottom, inducing focusing problems towards well borders, or location of cell within image, possibly affecting cellular features due to varying optical properties moving away from the image center (e.g. vignetting, decrease in sharpness, chromatic aberration, etc.). Biological sources of noise are even more plentiful and therefore increasingly harder to address. Obvious targets include general cell state parameters, such as cell cycle stage or whether the cell is apoptotic with the difficulty here being reliably determining these factor variables.

Work by \citeauthor{Snijder2012} elucidates the importance of cellular population context in cell-to-cell variability. In a comprehensive analysis of virally perturbed \gls{sirna} screens they demonstrate that parameters such as local cell density, population size and cell location within cellular aggregates significantly alter measured phenotypes. Building on these results, \citeauthor{Knapp2011} propose a normalization scheme by fitting a \gls{mars} model to a selection of features that represent the cellular population context (among other technical parameters) and using only the residuals for further analysis.

\Gls{mars} is an nonparametric regression procedure for finding a piecewise linear solution. No assumptions on data distributions are made and \gls{mars} represents a capable method for high-dimensional settings with respect to predictor variables. The following short introduction into \gls{mars} modeling is largely based on \cite{Hastie2009}. Basis functions of the form

\begin{equation}
  (x_j-t)_+ = \max(0, x_j-t) =
  \begin{cases}
    x_j-t,& \text{if } x_j > t\\
    0,              & \text{otherwise},
  \end{cases}
\end{equation}

and $(t-x_j)_+$, which are combined as reflected pairs, are used for describing the model surface. Such linear splines contain a knot at value $t$, separating the function support into a zero part and a nonzero domain, which for the reflected version are swapped with opposite slope. Despite each basis function only depending on a single covariate ($j \in \{1, 2, \dotsc, p\}$), they are considered as functions over the complete predictor space $\R^p$. Model building proceeds by maintaining two sets of reflected pairs, candidates and active pairs, where the active set initially contains only a constant term and candidates include all $2np$ possible functions with knots at each observed value $x_{ij}$

\begin{equation}
  C = \left\{(x_j-t)_+, (t-x_j)_+ \given t\in\{x_{1j}, x_{2j}, \dotsc, x_{nj}, \} \land j\in\{1, 2, \dotsc, p\} \right\}.
\end{equation}

A \gls{mars} model has the form

\begin{equation}
  g(x) = \mu + \sum_{m=1}^M \beta_m h_m(x),
\end{equation}

where the coefficients $\beta_m$ represent slopes of basis functions $h_m(\cdot)$ which can either be chosen from individual functions in C or by forming products of functions from the set of candidates C and thereby directly model interactions between variables. The active set is initialized with $A=\{h_0(x)=1\}$ and in an iterative procedure, for $k=1, 2, \dotsc, M$, the best pair of functions $\{h_{2k-1}(x), h_{2k}(x)\}$ with respect to the largest reduction in residual sum of squares is chosen and added to the active set, whereby the new additions are products of a reflected pair from the candidate set with a function $h_l(x)$ of the active set

\begin{subequations}
\begin{align}
  h_{2k-1}(x) &= h_l(x) \cdot (x_j-t)_+ \\
  h_{2k}(x) &= h_l(x) \cdot (t-x_j)_+.
\end{align}
\end{subequations}

The $2k$ coefficients are estimated by least squares and in each step the model grows by two basis functions. Termination of the iteration process occurs when a preset number of basis functions have been added to the active set, typically leading to a model that over-fits the data. A pruning scheme follows that from each pair of functions $\{h_{2k-1}(x), h_{2k}(x)\}$, removes the one that yields the smaller increase in residual sum of squares. In order to determine the right amount of backwards elimination and find the best model $\widehat{g}_\lambda^{\ *}(x)$, for each stage, a \gls{gcv} score is computed for the current model $\widehat{g}_\lambda(x)$. Another possibility is performing cross validation but this is often foregone due to computational expense. The \gls{gcv} criterion is defined as

\begin{equation}
  \text{gcv}(\lambda) = \frac{1}{n}\frac{\sum_{i=1}^n \left(y_i-\widehat{g}_\lambda(x_i)\right)^2}{\left(1-\frac{C(\lambda)}{n}\right)^2},
\end{equation}

where cost complexity term $C(\lambda)$ represents the effective number of parameters, computed as a sum of the number of linearly independent basis functions with the product of a smoothing parameter $d$ and the total number of terms. The value of $d$ is typically 2 (additive model) or 3 (higher orders allowed) and controls the amount of kinks introduced \citep{Friedman1991}.

By having the option of combining functions in the active set with newly entering terms, the algorithm builds a hierarchical model in the sense that interactions of added variables are only possible if all interacting partners are already present, forming an interaction of one order less. The reasoning behind this is that otherwise the search space would grow exponentially, causing computational issues for higher order interactions and in many cases it seems justifiable to require main effects as basis for interactions. Furthermore, the formation of powers is not allowed as a term may only enter a single time in an iteration, again limiting the search space in favor of computational efficiency. For interpretability and in larger problems for performance reasons, it is often advisable to restrict the number of interactions to degree 2 or 3.

\subsection{Normalization of Single Cell Data}
Implemented as part of singleCellFeatures, are both procedures for applying Z-scoring and B-scoring, as well as fitting a user-defined \gls{mars} model to each feature selected to be normalized (see section \ref{sec:scf-aug-norm}). Two sets of features so far have been used as predictors in \gls{mars}, a simpler, more conservative selection intended for targeting technical issues and a larger set based on the work of \cite{Knapp2011}, which includes population context.

\input{R/location-trends}

The smaller of the two contains predictors for object location within image and well, in addition to feature specific terms obtained through B-scoring. Motivated by findings as displayed in figure \ref{fig:data-location-trend}, cellular features are normalized using the locations of their respective nuclei. Figure \ref{fig:data-location-trend} shows a scatter-plot of mean actin intensity in cell objects versus their locations measured as Euclidean distance from the image center. The trend line (calculated by the function \mintinline{text}{gam} of the CRAN package mgcv) indicates an approximately linear relationship with negative slope. Such dependencies can reliably be found for most features, both at well and image level and their significance can be established by fitting multiple linear regression models. The resulting p-values for an overwhelming majority of features are highly significant, below machine accuracy ($<2\cdot10^{-16}$). As this effect constitutes an entirely experimental artifact, it seems reasonable to correct for location with respect to well and image centers.

Population context normalization additionally includes features for nucleus area, cell count per image, nuclear form factor, cell area, cell density, whether the cell is close to an image border and whether the cell is located at the edge of a colony. \citeauthor{Knapp2011} suggest that the procedure be carried out for a complete screen and while that is not a difficult task for the data they processed (only a single feature, 10-fold fewer cells), it is far more demanding for IndectX data. For an investigation considering only infected cells of a \textit{Brucella}, Dharmacon unpooled screen, this was necessary as only few cells per MTOR well are available which consequently have to be aggregated.

In order for the \gls{mars} procedure to even out technical effects as a normalization scheme spanning multiple plates, it is, at least for the dataset mentioned above, important to center data with respect to plate means prior to fitting the \gls{mars} model, as otherwise results are unsatisfactory (neither row, nor column effects can be completely removed, most probably due to underestimated plate effects). A further issue is that of memory. The entire screen consists of $2 \times 12$ plates which would require on the order of \SI{200}{\giga\byte} for storing the data alone, not taking operational overhead into account. This is infeasible, even for large memory machines. As MTOR wells are located on 8 of the 24 plates, only these were handled jointly.

Computationally, two avenues were explored, keeping all data in memory and utilizing disk scratch to lower the astronomical memory requirements of the former approach. Using a reasonably fast permanent storage device such as a \gls{ssd}, screen wide normalization is readily possible on regular desktop machines but frequent data fetching from storage incurs a significant time penalty. Processing 8 plates and \tilde 400 features takes on the order of \SI{36}{\hour}. Handling all data in memory speeds up the process to 6--\SI{8}{\hour}, but handling 8 plates, which alone only require \tilde \SI{60}{\giga\byte}, required a complete \SI{256}{\giga\byte} node of the ETH Euler cluster for processing.

Both the exclusively technical normalization procedure and the one including biological features additionally incorporate predictors obtained through B-scoring. For each of the features that are selected, the corresponding row, column and plate effects are calculated and are included with the mars model which consequently contains 5 predictors for the simple and 9 predictors for the more complex variant.

\input{R/normalization}

Figure \ref{fig:data-actin-normalization} is reproduced for providing some intuition on effects that may be addressed by the proposed normalization schemes. Panel (A) represents raw nuclear mean actin intensity densities of all 12 scrambled wells and the single MTOR well on a plate of the \textit{Brucella}, Dharmacon unpooled screen. Color coding of scrambled wells is based on the two groups that can easily be distinguished, which most probably are responsible for the alternating pattern of table \ref{tab:glm-1}. Wells that have a low column index are colored in shades of red, while late column wells are blue and colors grow darker with increasing row index. The density corresponding to MTOR is shown in black.

With \gls{mars} normalization, mainly due to B-scoring terms, the distance between the group centers are is drastically reduced without loosing information with respect to MTOR (panel B of figure \ref{fig:data-actin-normalization}). Likewise for variance scaling (Z-scoring without centering), the previously distinct amount of dispersion within the two scrambled groups is equalized without affecting MTOR (panel C). In order to make this step more robust, data is not divided by standard deviation, but by \gls{mad} and while being carried out on well level, not only the individual wells are used for calculating \gls{mad}, but the two vertical and horizontal neighbors are included as well (for plate borders, two neighboring wells on the same side of the target well are selected).

Finally, the two approaches can be coupled by first stabilizing dispersion, followed by \gls{mars} normalization (panel D). For this particular feature, the hybrid approach yields the best results in the sense that discrepancies between scrambled wells are reduced without loosing information on MTOR. Building on these results, one could assume that the combined normalization strategy should considerably improve \gls{glm} modeling. However, while \gls{mars} succeeds at recovering the data from column dependence, variance scaling does not help. Similarly for the two predictor sets in \gls{mars}, the more complicated one does not yield better data quality. Therefore, the most parsimonious normalization procedure constituting only of \gls{mars} targeted at technical issues is used to recreate table \ref{tab:glm-1} with corrected features as table \ref{tab:glm-2}.

\renewcommand{\arraystretch}{1.5}
\setlength{\tabcolsep}{0.25em}
\begin{table}
  \centering
  \caption[Reiteration of table \ref{tab:glm-1}, using normalized features for \gls{glm} fitting.]{For illustrating the effect of \gls{mars} normalization, using the smaller set of predictors targeted at technical issues only, this table is a reiteration of table \ref{tab:glm-1}, using normalized data instead. All other parameters (i.e. \gls{pca}, 90\% of variance, etc.) remain. The following models suffer from separated data: MTOR (A24, E2, E24, G1, G23, H2 and J2), PIK3R3 (H2), RIPK4 (A2, E24, G1, G23, H2, J2 and L1), Scrambled (G23 and H2) and TGFBR1 (E2, E24 and H24).}
  \label{tab:glm-2}
  \footnotesize
  \vspace{5px}
  \input{R/glm2}
\end{table}

When comparing table \ref{tab:glm-2} to  \ref{tab:glm-1}, the most striking difference is the disappearance of the striped pattern due to row dependence. This causes the previously very high prediction accuracies in scrambled wells to drop from \tilde 90\% to slightly more reasonable but still high \tilde 70\%. Furthermore, a clear difference between MTOR and scrambled rows is discernible. MTOR is consistently characterized by lower \gls{aic} (about half), much larger $\Delta\textsubscript{deviance}$ (2--6 fold difference) and better prediction scores, altogether indicating better modeling of the differences between MTOR and scrambled wells than of diversity within scrambled wells. This is, however, put somewhat into perspective by the other 3 genes that do not exhibit behavior that is easily distinguishable from scrambled.
% map pc back to feats
% how about ridge, w/o pca?

\section{Outlook and Conclusion}
Unfortunately, the goal that was originally set out to achieve, to find a set of influential features that discriminate single cell data of infected cells between an \gls{sirna} experiment targeting a hit gene and a scrambled control experiment, could so far not be accomplished. Such results may provide valuable insight into biological mechanisms as to how the down-regulated gene affects infection patterns, in turn, possibly yielding better understanding of pathogen infectivity in human cells. However, issues surrounding robust data normalization remain, despite much effort and prevent sensible inference from fitted models to be drawn. With current conditioning schemes, discrepancies between control wells persist to such an extent that biologically equivalent data can be separated witch 60--70\% accuracy (cf table \ref{tab:glm-2}), which is a clear indication of technical effects still being present to an unacceptable degree. Furthermore, the extent of inconsistencies among scrambled control wells is on the same order of magnitude as differences between wells containing \gls{sirna} sequences against hit genes (with the exception of MTOR), thereby making any possible list of influential features derived from current data highly questionable.

Issues of reproducibility have been plaguing \gls{sirna} based \gls{hts} ever since its conception and have recently gained attention with researchers calling for standardization of screening practice and more robust hit scoring. An example of the issue is provided by four RNAi screens investigating \gls{hiv} infection, performed in 2008 and 2009, three of which employ pooled \gls{sirna} duplexes while the fourth uses pooled \gls{shrna} reagents. On average, each of the screens reported 300 hit genes and interestingly, there is zero overlap among all four and only three genes are shared throughout the three \gls{sirna} screens \citep{Bhinder2014}.

A large-scale study provided by \cite{Bhinder2013} corroborates concerns over reproducibility and quality of obtained hits. Out of a reviewed list of 300 published screening experiments, 30 lethality-based screens, performed by different research groups, were selected for comparative investigation. Aggregation of hits implicates a third of the overall genome, suggesting that an unlikely 30\% of all genes are essential to cellular survival. When investigating commonalities, not a single gene is reported throughout all screens. Constraining analysis to the 16 \gls{sirna} screens (the other 14 use \gls{shrna}), leaves two genome-wide and 14 focused studies and for both groups, 90\% of reported hits are specific to a single screen (orphan hits), while 4 genes are shared among the two genome wide and none are common to all focused experiments.

Such observations are mainly concerned with comparing the results of distinct investigations performed by separate research groups, possibly using individual procedures and a large source of variation must be attributed to the current lack of standard practice. Nevertheless, for utilizing \gls{sirna} based \gls{hts} at its full potential, issues of reproducibility must be resolved. To that end, much effort by InfectX has been dedicated to the development of a robust screening platform, providing as many replicates as possible and making data available publicly (a serious reproducibility issue is the nonavailability of raw data as it prevents others from examining and potentially improving data processing). Nevertheless, many technical artifacts, sources of variation intrinsic to \gls{sirna} based experimentation, as well as the noisy nature of biological systems have to be handled.

While a fair amount of scrutiny towards data quality at well level, has led to some normalization procedures being developed for corresponding datasets, less work so far has been dedicated to the single cell level. As demonstrated, a combination of Z-scoring, B-scoring and a \gls{mars} model as proposed by \cite{Knapp2011} is not sufficient for making single cell data of distinct wells directly comparable. Whether there are further technical effects that are not removed by this set of methods or biological issues that have not been considered are to blame is an open question. Further, it is unclear if correcting for population context is beneficial to the type of investigation at the heart of this project, as it might be overzealous in that cellular feature information that originates from \gls{sirna} perturbations and manifests within population context is possibly removed.

\begin{figure}
  \centering
  \includegraphics[width=0.95\textwidth]{sirna-variablilty}
  \caption[Feature space variability of different \gls{sirna} sequences sharing the same target gene.]{A total of 5 distinct \gls{sirna} sequences with the shared target talin 1, required for invasome formation in \textit{Bartonella} infection, yield considerable variability in feature space. While there are some commonalities, due to on-target effects, off targeting may cause individual behavior patterns. Figure take from \cite{Geier2010}.}
  \label{fig:sirna-variablilty}
\end{figure}

Instead of targeting population context related information, sources of variation such as \glspl{ote} might provide more attractive objectives. Off-targeting has been established as an important confounding factor in \gls{sirna} screening and it is entirely possible that some of the variability among scrambled control experiments is attributable to unintentional gene silencing. While scrambled seed sequences are specifically engineered not to match any known genes, especially \glspl{3-utr}, the limited combinatorial space makes some degree of interaction likely. Furthermore, unspecific response may occur and add to the obfuscation of the actual signal. It might be possible to adapt recent developments in deconvolution of off-target confounded RNAi screens, such as provided by \cite{Schmich2015} to the single cell level.

When only focusing on infected cells, the importance of correcting for \glspl{ote} increases as wells of different sequences with the same target have to be aggregated in cases where the number of infected cells per single well is low (as is the case in \textit{Brucella} screens). The same holds for pooled \gls{sirna} screens, as several nonidentical sequences with a common target are included in the same experiment. Distinguishing the signal of target knockdown from random perturbations constitutes a valuable improvement in such situations.

In addition to accounting for \glspl{ote}, the present normalization scheme could be adapted to treat features individually, instead of repeatedly being applied in the same manner. There is great diversity among the multitude of features that are extracted from imagery which warrants further study. Perhaps individual data transformations need be applied and it may be necessary, albeit labor intensive to customize normalization at feature level. This was conceded but due to limited time availability so far could not be explored further.

If sufficient normalization proves elusive, changing the analysis setup has to be contemplated. Instead of determining influential features by comparing wells, modeling infected versus uninfected cells within a well might yield the desired insights. This alleviates the requirement of directly comparable wells while still determining well specific parameters. The features used for infection scoring are fixed pathogen wide and finding gene specific sets constitutes information that is qualitatively similar to that of the original investigation.

Finally, the number of available features is larger than the number of features considered during analysis and modeling. So far, only cellular features are included that are scalar valued per cell and provide a data point for each cell. Therefore pathogen objects and neighbor features are left out. This additional information could be included by developing appropriate augmentation functions such as location augmentation as implemented by singleCellFeatures. In that particular case, object coordinates which by themselves do not convey any directly interpretable information are transformed to more useful features, including local object density, distance from center, location within colonies, etc. Similarly, neighbor identity, again by itself not very useful, could be exploited to define object clusters and determine cluster area, cluster size in terms of members, number of infected members, and many other derived features. Adjacency matrices are assembled by singleCellFeatures and await being put to use.

While the desired list of influential features for \gls{sirna} perturbed infection so far could not be compiled, a flexible and capable platform for manipulating the large amounts of data associated with single cell level, high throughput screening, was developed. More sophisticated normalization procedures have to be investigated and hopefully, building on lessons learnt from data analysis provided by this project, ways of extracting the initially targeted information from available datasets will be found in the future.
