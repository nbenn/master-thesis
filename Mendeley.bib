@book{Buehlmann2011,
	author = {B\"{u}hlmann, Peter and van de Geer, Sara},
	edition = {1st},
	file = {:Users/nbennett/Dropbox/Personal/Mendeley/Files/B\"{u}hlmann, van de Geer/Statistics for High-Dimensional Data Methods, Theory and Applications.pdf:pdf;:Users/nbennett/Dropbox/Personal/Mendeley/Files/B\"{u}hlmann, van de Geer/Statistics for High-Dimensional Data Methods, Theory and Applications(2).pdf:pdf;:Users/nbennett/Dropbox/Personal/Mendeley/Files/B\"{u}hlmann, van de Geer/Statistics for High-Dimensional Data Methods, Theory and Applications(3).pdf:pdf;:Users/nbennett/Dropbox/Personal/Mendeley/Files/B\"{u}hlmann, van de Geer/Statistics for High-Dimensional Data Methods, Theory and Applications(4).pdf:pdf;:Users/nbennett/Dropbox/Personal/Mendeley/Files/B\"{u}hlmann, van de Geer/Statistics for High-Dimensional Data Methods, Theory and Applications(5).pdf:pdf;:Users/nbennett/Dropbox/Personal/Mendeley/Files/B\"{u}hlmann, van de Geer/Statistics for High-Dimensional Data Methods, Theory and Applications(6).pdf:pdf;:Users/nbennett/Dropbox/Personal/Mendeley/Files/B\"{u}hlmann, van de Geer/Statistics for High-Dimensional Data Methods, Theory and Applications(7).pdf:pdf;:Users/nbennett/Dropbox/Personal/Mendeley/Files/B\"{u}hlmann, van de Geer/Statistics for High-Dimensional Data Methods, Theory and Applications(8).pdf:pdf;:Users/nbennett/Dropbox/Personal/Mendeley/Files/B\"{u}hlmann, van de Geer/Statistics for High-Dimensional Data Methods, Theory and Applications(9).pdf:pdf;:Users/nbennett/Dropbox/Personal/Mendeley/Files/B\"{u}hlmann, van de Geer/Statistics for High-Dimensional Data Methods, Theory and Applications(10).pdf:pdf;:Users/nbennett/Dropbox/Personal/Mendeley/Files/B\"{u}hlmann, van de Geer/Statistics for High-Dimensional Data Methods, Theory and Applications(11).pdf:pdf;:Users/nbennett/Dropbox/Personal/Mendeley/Files/B\"{u}hlmann, van de Geer/Statistics for High-Dimensional Data Methods, Theory and Applications(12).pdf:pdf;:Users/nbennett/Dropbox/Personal/Mendeley/Files/B\"{u}hlmann, van de Geer/Statistics for High-Dimensional Data Methods, Theory and Applications(13).pdf:pdf;:Users/nbennett/Dropbox/Personal/Mendeley/Files/B\"{u}hlmann, van de Geer/Statistics for High-Dimensional Data Methods, Theory and Applications(14).pdf:pdf;:Users/nbennett/Dropbox/Personal/Mendeley/Files/B\"{u}hlmann, van de Geer/Statistics for High-Dimensional Data Methods, Theory and Applications(15).pdf:pdf;:Users/nbennett/Dropbox/Personal/Mendeley/Files/B\"{u}hlmann, van de Geer/Statistics for High-Dimensional Data Methods, Theory and Applications(16).pdf:pdf},
	isbn = {3642201911, 9783642201912},
	publisher = {Springer Publishing Company, Incorporated},
	title = {{Statistics for High-Dimensional Data: Methods, Theory and Applications}},
	url = {http://link.springer.com/book/10.1007/978-3-642-20192-9},
	year = 2011
}

@article{Gelman2008,
	abstract = {We propose a new prior distribution for classical (nonhierarchical) logistic regression models, constructed by first scaling all nonbinary variables to have mean 0 and standard deviation 0.5, and then placing independent Student-t prior distributions on the coefficients. As a default choice, we recommend the Cauchy distribution with center 0 and scale 2.5, which in the simplest setting is a longer-tailed version of the distribution attained by assuming one-half additional success and one-half additional failure in a logistic regression. Cross-validation on a corpus of datasets shows the Cauchy class of prior distributions to outperform existing implementations of Gaussian and Laplace priors. We recommend this prior distribution as a default choice for routine applied use. It has the advantage of always giving answers, even when there is complete separation in logistic regression (a common problem, even when the sample size is large and the number of predictors is small), and also automatically applying more shrinkage to higher-order interactions. This can be useful in routine data analysis as well as in automated procedures such as chained equations for missing-data imputation. We implement a procedure to fit generalized linear models in R with the Student-t prior distribution by incorporating an approximate EM algorithm into the usual iteratively weighted least squares. We illustrate with several applications, including a series of logistic regressions predicting voting preferences, a small bioassay experiment, and an imputation model for a public health data set.},
	author = {Gelman, Andrew and Jakulin, Aleks and Pittau, Maria Grazia and Su, Yu-Sung},
	doi = {10.1214/08-AOAS191},
	journal = {Ann. Appl. Stat.},
	number = 4,
	pages = {1360--1383},
	publisher = {The Institute of Mathematical Statistics},
	title = {{A weakly informative default prior distribution for logistic and other regression models}},
	url = {http://dx.doi.org/10.1214/08-AOAS191},
	volume = 2,
	year = 2008
}

@book{Hastie2009,
	abstract = {During the past decade there has been an explosion in computation and information technology. With it has come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It should be a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting-the first comprehensive treatment of this topic in any book. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie wrote much of the statistical modeling software in S-PLUS and invented principal curves and surfaces. Tibshirani proposed the Lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, and projection pursuit. FROM THE REVIEWS: TECHNOMETRICS "This is a vast and complex book. Generally, it concentrates on explaining why and how the methods work, rather than how to use them. Examples and especially the visualizations are principle features...As a source for the methods of statistical learning...it will probably be a long time before there is a competitor to this book."},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	booktitle = {Elements},
	doi = {10.1007/b94608},
	file = {:Users/nbennett/Dropbox/Personal/Mendeley/Files/Hastie, Tibshirani, Friedman/The Elements of Statistical Learning.pdf:pdf},
	isbn = 9780387848570,
	issn = 03436993,
	pages = {337--387},
	pmid = 15512507,
	title = {{The Elements of Statistical Learning}},
	url = {http://www.springerlink.com/index/10.1007/b94608},
	volume = 1,
	year = 2009
}

@phdthesis{Konis2007,
	abstract = {This thesis is a study of the detection of separation among the sample points in binary logistic regression models. We propose a new algorithm for detecting separation and demonstrate empirically that it can be computed fast enough to be used routinely as part of the fitting process for logistic regression models. The parameter estimates of a binary logistic regression model fit using the method of maximum likelihood sometimes do not converge to finite values. This phenomenon (also known as monotone likelihood or infinite parameters) occurs because of a condition among the sample points known as separation. There are two classes of separation. When complete separation is present among the sample points$\backslash$, iterative procedures for maximizing the likelihood tend to break down$\backslash$, when it would be clear that there is a problem with the model. However$\backslash$, when quasicomplete separation is present among the sample points$\backslash$, the iterative procedures for maximizing the likelihood tend to satisfy their convergence criterion before revealing any indication of separation. The new algorithm is based on a linear program with a nonnegative objective function that has a positive optimal value when separation is present among the sample points. We compare several approaches for solving this linear program and find that a method based on determining the feasibility of the dual to this linear program provides a numerically reliable test for separation among the sample points. A simulation study shows that this test can be computed in a similar amount of time as fitting the binary logistic regression model using the method of iteratively reweighted least squares: hence the test is fast enough to be used routinely as part of the fitting procedure. An implementation of our algorithm (as well as the other methods described in this thesis) is available in the R package safeBinaryRegression.},
	author = {Konis, Kjell Peter},
	file = {:Users/nbennett/Dropbox/Personal/Mendeley/Files/Konis/Linear programming algorithms for detecting separated data in binary logistic regression models.pdf:pdf},
	keywords = {binary data,infinite parameters,linear programming,logistic regression,monotone likelihood},
	school = {University of Oxford},
	tags = {binary data,infinite parameters,linear programming,logistic regression,monotone likelihood},
	title = {{Linear programming algorithms for detecting separated data in binary logistic regression models}},
	type = {Doctoral},
	url = {http://ora.ouls.ox.ac.uk/objects/uuid:8f9ee0d0-d78e-4101-9ab4-f9cbceed2a2a},
	year = 2007
}

@book{McCullagh1989,
	author = {McCullagh, P and Nelder, J A},
	isbn = 9780412317606,
	keywords = {GLM reference statistics},
	publisher = {Chapman \& Hall},
	series = {Chapman and Hall/CRC Monographs on Statistics and Applied Probability Series},
	title = {{Generalized Linear Models, Second Edition}},
	url = {http://books.google.com/books?id=h9kFH2\_FfBkC},
	year = 1989
}
